{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import GPT2ForSequenceClassification, GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform\n",
    "platform.platform()\n",
    "\n",
    "torch.has_mps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Call_Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jung c g carl gustav 18751961 psychoanalysis</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>witchcraft  germany  braunschweig region demon...</td>\n",
       "      <td>1583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>psychology  qualitative research ethnology  me...</td>\n",
       "      <td>76.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>selling psychology applied</td>\n",
       "      <td>636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>associations institutions etc stress psycholog...</td>\n",
       "      <td>175.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title Call_Number\n",
       "0       jung c g carl gustav 18751961 psychoanalysis         174\n",
       "1  witchcraft  germany  braunschweig region demon...        1583\n",
       "2  psychology  qualitative research ethnology  me...        76.5\n",
       "3                         selling psychology applied         636\n",
       "4  associations institutions etc stress psycholog...       175.5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path = 'Data/BF_Subject_Clean_Call_Number.csv'\n",
    "df = pd.read_csv(train_data_path, sep='\\t')\n",
    "df = df.drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658\n",
      "{'174': 0, '1583': 1, '76.5': 2, '636': 3, '175.5': 4, '121': 5, '723': 6, '1598': 7, '210': 8, '870': 9, '711': 10, '698.5': 11, '201': 12, '323': 13, '311': 14, '713': 15, '319': 16, '431': 17, '176': 18, '637': 19, '18.02': 20, '175': 21, '633': 22, '632.5': 23, '1576': 24, '173': 25, '710': 26, '698': 27, '378': 28, '697': 29, '1325': 30, '408': 31, '432': 32, '113': 33, '325': 34, '448': 35, '639': 36, '295': 37, '371': 38, '207': 39, '721': 40, '1593': 41, '692.2': 42, '1407': 43, '724': 44, '575': 45, '318': 46, '1779': 47, '341': 48, '1999': 49, '692.5': 50, '162': 51, '39': 52, '321': 53, '441': 54, '38': 55, '714': 56, '1622': 57, '447': 58, '109': 59, '76.4': 60, '1791': 61, '241': 62, '1611': 63, '698.3': 64, '316.6': 65, '1561': 66, '105': 67, '315': 68, '701': 69, '335': 70, '698.95': 71, '455': 72, '1793': 73, '201.3': 74, '38.5': 75, '724.3': 76, '1434': 77, '233': 78, '774': 79, '720': 80, '717': 81, '458': 82, '531': 83, '81': 84, '31': 85, '1591': 86, '39.9': 87, '1241': 88, '1714': 89, '698.8': 90, '318.5': 91, '692': 92, '503': 93, '18.022': 94, '412': 95, '1031': 96, '1141': 97, '698.4': 98, '1584': 99, '346': 100, '1261.2': 101, '511': 102, '1242': 103, '1729': 104, '1623': 105, '1701': 106, '671': 107, '242': 108, '319.5': 109, '171': 110, '1559': 111, '175.4': 112, '1556': 113, '1472': 114, '1534': 115, '1028.5': 116, '204': 117, '449': 118, '1520': 119, '724.5': 120, '367': 121, '683': 122, '161': 123, '1581': 124, '468': 125, '199': 126, '1567': 127, '1566': 128, '722': 129, '697.5': 130, '1301': 131, '293': 132, '77': 133, '611': 134, '416': 135, '1078': 136, '445': 137, '444': 138, '209': 139, '1042': 140, '125': 141, '1671': 142, '353': 143, '1548': 144, '456': 145, '1275': 146, '108': 147, '1615': 148, '1770': 149, '76': 150, '724.55': 151, '1321': 152, '724.8': 153, '719': 154, '1762': 155, '871': 156, '1681': 157, '713.5': 158, '1571': 159, '469': 160, '905': 161, '57': 162, '481': 163, '724.85': 164, '182': 165, '1546': 166, '1517': 167, '431.5': 168, '423': 169, '1045': 170, '636.6': 171, '636.64': 172, '95': 173, '1571.5': 174, '1768': 175, '818': 176, '51': 177, '1021': 178, '1385': 179, '80.8': 180, '198.7': 181, '1152': 182, '411': 183, '632': 184, '204.5': 185, '376': 186, '21': 187, '789': 188, '1283': 189, '1411': 190, '442': 191, '1558': 192, '204.6': 193, '773': 194, '698.9': 195, '698.7': 196, '1589': 197, '41': 198, '1261': 199, '237': 200, '128': 201, '1311': 202, '353.5': 203, '1676': 204, '1815': 205, '317': 206, '898': 207, '315.5': 208, '1531': 209, '1555': 210, '1773.2': 211, '181': 212, '149': 213, '1051': 214, '638': 215, '88': 216, '1775': 217, '719.5': 218, '76.7': 219, '1613': 220, '1091': 221, '435': 222, '131': 223, '505': 224, '1761': 225, '1389': 226, '1575': 227, '1765': 228, '868': 229, '204.7': 230, '385': 231, '314': 232, '1468': 233, '724.6': 234, '433': 235, '76.6': 236, '327': 237, '183': 238, '365': 239, '123': 240, '2050': 241, '636.7': 242, '1281': 243, '640': 244, '432.5': 245, '39.5': 246, '1724': 247, '809.8': 248, '619.5': 249, '798': 250, '395': 251, '504': 252, '76.8': 253, '1251': 254, '200': 255, '1425': 256, '495': 257, '122': 258, '463': 259, '1098': 260, '1601': 261, '251': 262, '1674': 263, '552': 264, '1025': 265, '1175': 266, '1439': 267, '1522': 268, '1040': 269, '1582': 270, '708': 271, '152': 272, '231': 273, '370': 274, '151': 275, '1503': 276, '1099': 277, '316': 278, '1142': 279, '261': 280, '851': 281, '1777': 282, '357': 283, '576': 284, '1569': 285, '275': 286, '1752': 287, '636.67': 288, '561': 289, '426': 290, '176.5': 291, '110': 292, '698.35': 293, '1868': 294, '1718': 295, '636.3': 296, '1032': 297, '1854': 298, '103': 299, '1563': 300, '64': 301, '1600': 302, '833': 303, '636.68': 304, '1771': 305, '1708.1': 306, '1027': 307, '1573': 308, '201.4': 309, '1565': 310, '635': 311, '245': 312, '133': 313, '1071': 314, '443': 315, '118': 316, '703': 317, '39.2': 318, '1127': 319, '1711': 320, '1795': 321, '591': 322, '80.7': 323, '551': 324, '621': 325, '175.45': 326, '1156': 327, '619': 328, '1462': 329, '75': 330, '449.5': 331, '1728': 332, '11': 333, '1751': 334, '673': 335, '648': 336, '722.3': 337, '928': 338, '482': 339, '1997': 340, '869': 341, '662': 342, '592': 343, '832': 344, '1586': 345, '1879': 346, '175.7': 347, '908': 348, '1612': 349, '39.4': 350, '515': 351, '22': 352, '474': 353, '76.3': 354, '313': 355, '20': 356, '588': 357, '698.2': 358, '471': 359, '203': 360, '778': 361, '1708.2': 362, '727': 363, '724.65': 364, '30': 365, '1683': 366, '1033': 367, '98': 368, '467': 369, '446': 370, '1805': 371, '622': 372, '1404': 373, '712.7': 374, '761': 375, '437': 376, '111': 377, '1429': 378, '891': 379, '731': 380, '56': 381, '40': 382, '1655': 383, '337': 384, '44': 385, '114': 386, '1410': 387, '1171': 388, '755': 389, '630': 390, '645': 391, '734': 392, '1504': 393, '1769': 394, '1679': 395, '145': 396, '141': 397, '176.2': 398, '1713': 399, '172': 400, '475': 401, '1588': 402, '174.4': 403, '1572': 404, '91': 405, '1290': 406, '163': 407, '641': 408, '702': 409, '1413': 410, '1680': 411, '1602': 412, '202': 413, '623': 414, '140': 415, '1412': 416, '1685': 417, '23': 418, '1874': 419, '39.3': 420, '1461': 421, '432.3': 422, '859': 423, '1463': 424, '1331': 425, '565': 426, '32': 427, '1003': 428, '1408.2': 429, '1552': 430, '1702': 431, '1029': 432, '1994': 433, '1038': 434, '1651': 435, '78': 436, '1040.5': 437, '1305': 438, '1408': 439, '1618': 440, '1409': 441, '1861': 442, '1421': 443, '1473': 444, '1445': 445, '1792': 446, '1628': 447, '1878': 448, '1597': 449, '1521': 450, '1533': 451, '1505': 452, '1501': 453, '1691': 454, '1661': 455, '1715': 456, '921': 457, '1291': 458, '1262': 459, '1228': 460, '1272': 461, '1277': 462, '1235': 463, '1378': 464, '1125': 465, '831': 466, '1786': 467, '892': 468, '1343': 469, '893': 470, '852': 471, '187': 472, '573': 473, '1075': 474, '521': 475, '1416': 476, '879': 477, '1721': 478, '1023': 479, '191': 480, '712.5': 481, '1595': 482, '7271': 483, '1442': 484, '853': 485, '885': 486, '1288': 487, '67': 488, '139': 489, '699': 490, '79': 491, '685': 492, '1475': 493, '192': 494, '39.8': 495, '292': 496, '1812': 497, '940': 498, '563': 499, '533': 500, '320': 501, '381': 502, '309': 503, '613': 504, '625': 505, '1211': 506, '825': 507, '1068': 508, '1146': 509, '1143': 510, '1132': 511, '1131': 512, '6985': 513, '188': 514, '418': 515, '733': 516, '732': 517, '751': 518, '676': 519, '672': 520, '1': 521, '634': 522, '1773': 523, '1080': 524, '410': 525, '501': 526, '1028': 527, '1026': 528, '1263': 529, '1471': 530, '1577': 531, '47': 532, '491': 533, '315.2': 534, '595': 535, '592.5': 536, '1052': 537, '582': 538, '1809': 539, '1708.8': 540, '76.72': 541, '605': 542, '693': 543, '1339': 544, '17': 545, '753': 546, '1708.5': 547, '13': 548, '1414': 549, '578': 550, '80.7.A7': 551, '1726': 552, '532': 553, '1596': 554, '1750': 555, '724.2': 556, '1286': 557, '80.7.A78': 558, '663': 559, '501.5': 560, '184': 561, '721.3': 562, '1587': 563, '1679.8': 564, '1621': 565, '150': 566, '1723': 567, '93': 568, '205': 569, '1265': 570, '719.6': 571, '241.7': 572, '375': 573, '112': 574, '858': 575, '294': 576, '93.2': 577, '299': 578, '746': 579, '1758': 580, '698.6': 581, '1550': 582, '107': 583, '132': 584, '1785': 585, '1371': 586, '1553': 587, '636.65': 588, '712': 589, '692.15': 590, '698.55': 591, '1466': 592, '243': 593, '844': 594, '251.6': 595, '1083': 596, '190': 597, '271': 598, '1578': 599, '317.5': 600, '1717.5': 601, '79.5': 602, '1095': 603, '127': 604, '538': 605, '201.5': 606, '811': 607, '1523': 608, '1603': 609, '1381': 610, '901': 611, '724.25': 612, '604': 613, '1444': 614, '692.9': 615, '1801': 616, '505.6': 617, '76.45': 618, '783': 619, '497': 620, '740': 621, '86': 622, '1405': 623, '1030': 624, '1483': 625, '1628.3': 626, '1608': 627, '1716': 628, '1001': 629, '1347': 630, '1352': 631, '1418': 632, '1213': 633, '1092': 634, '821': 635, '431.3': 636, '775': 637, '149.8': 638, '138': 639, '126': 640, '660': 641, '285': 642, '186': 643, '935': 644, '351': 645, '620': 646, '380': 647, '1148': 648, '1121': 649, '1128': 650, '724.55.C63': 651, '728': 652, '115': 653, '4705': 654, '647': 655, '1891': 656, '1524': 657}\n"
     ]
    }
   ],
   "source": [
    "vals = pd.unique(df['Call_Number'])\n",
    "print(len(vals))\n",
    "labels = {value: key for (key, value) in enumerate(vals)}\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "class dataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, df):\n",
    "\n",
    "    \n",
    "\n",
    "    self.labels = [labels[label] for label in df['Call_Number']]\n",
    "    self.texts = [tokenizer(text,\n",
    "                            padding='max_length',max_length=512, truncation=True,\n",
    "                            return_tensors=\"pt\") for text in df['Title']]\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.labels)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    batch_texts = self.get_batch_texts(idx)\n",
    "    batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "    return batch_texts, batch_y\n",
    "  def classes(self):\n",
    "    return self.labels\n",
    "\n",
    "  def get_batch_labels(self, idx):\n",
    "    return np.array(self.labels[idx])\n",
    "\n",
    "  def get_batch_texts(self, idx):\n",
    "    return self.texts[idx]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "15074 1884 1885\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "rand = np.random.randint(1,high=100)\n",
    "print(rand)\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=rand),\n",
    "                                     [int(.8*len(df)), int(.9*len(df))])\n",
    "\n",
    "print(len(df_train), len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Call_Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18255</th>\n",
       "      <td>attitude psychology dogmatism</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16478</th>\n",
       "      <td>psychology  history</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18594</th>\n",
       "      <td>goal psychology motivation psychology affect p...</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17029</th>\n",
       "      <td>psychological tests psychometrics</td>\n",
       "      <td>38.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15840</th>\n",
       "      <td>worth patience spirit parapsychology</td>\n",
       "      <td>1301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7980</th>\n",
       "      <td>psychological tests</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17370</th>\n",
       "      <td>thought and thinking</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10496</th>\n",
       "      <td>identity psychology identity psychology in lit...</td>\n",
       "      <td>697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8155</th>\n",
       "      <td>conduct of life  early works to 1800 emotions ...</td>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17024</th>\n",
       "      <td>time perception</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15074 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title Call_Number\n",
       "18255                      attitude psychology dogmatism         378\n",
       "16478                                psychology  history          81\n",
       "18594  goal psychology motivation psychology affect p...         504\n",
       "17029                  psychological tests psychometrics        38.5\n",
       "15840               worth patience spirit parapsychology        1301\n",
       "...                                                  ...         ...\n",
       "7980                                 psychological tests         431\n",
       "17370                               thought and thinking         455\n",
       "10496  identity psychology identity psychology in lit...         697\n",
       "8155   conduct of life  early works to 1800 emotions ...         551\n",
       "17024                                    time perception         468\n",
       "\n",
       "[15074 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, learning_rate, epoch):\n",
    "  training_dataset, val_dataset = dataset(train_data), dataset(val_data)\n",
    "\n",
    "  train_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=2, shuffle=True)\n",
    "  val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=2)\n",
    "\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "  optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  prev_val_acc = 0\n",
    "  \n",
    "\n",
    "  if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "  \n",
    "  for epoch_num in range(epoch):\n",
    "\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "\n",
    "    for train_input, train_label in tqdm(train_dataloader):\n",
    "      \n",
    "\n",
    "      train_label = train_label.to(device)\n",
    "      mask = train_input['attention_mask'].to(device)\n",
    "      input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "      model.zero_grad()\n",
    "\n",
    "      outputs = model(input_ids=input_id, attention_mask=mask, labels=train_label)\n",
    "      loss = outputs.loss\n",
    "      logits = outputs.logits\n",
    "\n",
    "      total_loss_train += loss.item()\n",
    "      acc = (logits.argmax(axis=-1) == train_label).sum().item()\n",
    "      total_acc_train += acc\n",
    "\n",
    "      loss.backward()\n",
    "      #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "      optimizer.step()\n",
    "\n",
    "    \n",
    "    total_acc_val = 0\n",
    "    total_loss_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      print(\"validating\")\n",
    "      for val_input, val_label in val_dataloader:\n",
    "        val_label = val_label.to(device)\n",
    "        mask = val_input['attention_mask'].to(device)\n",
    "        input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_id, attention_mask=mask, labels=val_label)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        total_loss_val += loss.item()\n",
    "        acc = (logits.argmax(axis=-1) == val_label).sum().item()\n",
    "        total_acc_val += acc\n",
    "\n",
    "    print(\n",
    "        f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "        | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "        | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "        | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "    \n",
    "    if prev_val_acc > total_acc_val / len(val_data):\n",
    "      print(\"Breaking early because validation accuracy has decreased\")\n",
    "      break\n",
    "    else:\n",
    "      prev_val_Acc = total_acc_val / len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "  test = dataset(test_data)\n",
    "\n",
    "  test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
    "\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "  if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "  total_acc_test = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for test_input, test_label in test_dataloader:\n",
    "      test_label = test_label.to(device)\n",
    "      mask = test_input['attention_mask'].to(device)\n",
    "      input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "      \n",
    "      outputs = model(input_ids=input_id, attention_mask=mask, labels=test_label)\n",
    "      loss = outputs.loss\n",
    "      logits = outputs.logits\n",
    "      acc = (logits.argmax(axis=-1) == test_label).sum().item()\n",
    "      total_acc_test += acc\n",
    "  \n",
    "  print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'model' in locals():\n",
    "  del model\n",
    "\n",
    "if 'configuration' in locals():\n",
    "  del configuration\n",
    "\n",
    "configuration = GPT2Config()\n",
    "configuration.num_labels = len(vals)\n",
    "\n",
    "model = GPT2ForSequenceClassification(configuration)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7537 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/gregoryknapp/Desktop/BookshelfProject/BookClassification/GPT2Classification.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gregoryknapp/Desktop/BookshelfProject/BookClassification/GPT2Classification.ipynb#ch0000013?line=0'>1</a>\u001b[0m LR \u001b[39m=\u001b[39m \u001b[39m1e-6\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gregoryknapp/Desktop/BookshelfProject/BookClassification/GPT2Classification.ipynb#ch0000013?line=1'>2</a>\u001b[0m EPOCHS \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gregoryknapp/Desktop/BookshelfProject/BookClassification/GPT2Classification.ipynb#ch0000013?line=2'>3</a>\u001b[0m train(model, df_train, df_val, LR, EPOCHS)\n",
      "\u001b[1;32m/Users/gregoryknapp/Desktop/BookshelfProject/BookClassification/GPT2Classification.ipynb Cell 11'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data, val_data, learning_rate, epoch)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gregoryknapp/Desktop/BookshelfProject/BookClassification/GPT2Classification.ipynb#ch0000009?line=29'>30</a>\u001b[0m input_id \u001b[39m=\u001b[39m train_input[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gregoryknapp/Desktop/BookshelfProject/BookClassification/GPT2Classification.ipynb#ch0000009?line=31'>32</a>\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gregoryknapp/Desktop/BookshelfProject/BookClassification/GPT2Classification.ipynb#ch0000009?line=33'>34</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39;49minput_id, attention_mask\u001b[39m=\u001b[39;49mmask, labels\u001b[39m=\u001b[39;49mtrain_label)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gregoryknapp/Desktop/BookshelfProject/BookClassification/GPT2Classification.ipynb#ch0000009?line=34'>35</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gregoryknapp/Desktop/BookshelfProject/BookClassification/GPT2Classification.ipynb#ch0000009?line=35'>36</a>\u001b[0m logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1131\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1130\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1132\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1378\u001b[0m, in \u001b[0;36mGPT2ForSequenceClassification.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1371\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1372\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1376\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1378\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1379\u001b[0m     input_ids,\n\u001b[1;32m   1380\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1381\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1382\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1383\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1384\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1385\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1386\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1387\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1388\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1389\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1390\u001b[0m )\n\u001b[1;32m   1391\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1392\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscore(hidden_states)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1131\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1130\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1132\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:834\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    831\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mn_layer)\n\u001b[1;32m    833\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 834\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwte(input_ids)\n\u001b[1;32m    835\u001b[0m position_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwpe(position_ids)\n\u001b[1;32m    836\u001b[0m hidden_states \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m position_embeds\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1131\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1130\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1132\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/torch/nn/modules/sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    159\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/torch/nn/functional.py:2155\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2149\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2150\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2151\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2152\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2153\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2154\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2155\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "LR = 1e-6\n",
    "EPOCHS = 5\n",
    "train(model, df_train, df_val, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, df_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
