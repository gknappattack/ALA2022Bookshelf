{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'macOS-12.4-arm64-i386-64bit'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform\n",
    "platform.platform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.has_mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Call_Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jung c g carl gustav 18751961 psychoanalysis</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>witchcraft  germany  braunschweig region demon...</td>\n",
       "      <td>1583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>psychology  qualitative research ethnology  me...</td>\n",
       "      <td>76.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>selling psychology applied</td>\n",
       "      <td>636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>associations institutions etc stress psycholog...</td>\n",
       "      <td>175.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title Call_Number\n",
       "0       jung c g carl gustav 18751961 psychoanalysis         174\n",
       "1  witchcraft  germany  braunschweig region demon...        1583\n",
       "2  psychology  qualitative research ethnology  me...        76.5\n",
       "3                         selling psychology applied         636\n",
       "4  associations institutions etc stress psycholog...       175.5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path = 'Data/BF_Subject_Clean_Call_Number.csv'\n",
    "df = pd.read_csv(train_data_path, sep='\\t')\n",
    "df = df.drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658\n",
      "{'174': 0, '1583': 1, '76.5': 2, '636': 3, '175.5': 4, '121': 5, '723': 6, '1598': 7, '210': 8, '870': 9, '711': 10, '698.5': 11, '201': 12, '323': 13, '311': 14, '713': 15, '319': 16, '431': 17, '176': 18, '637': 19, '18.02': 20, '175': 21, '633': 22, '632.5': 23, '1576': 24, '173': 25, '710': 26, '698': 27, '378': 28, '697': 29, '1325': 30, '408': 31, '432': 32, '113': 33, '325': 34, '448': 35, '639': 36, '295': 37, '371': 38, '207': 39, '721': 40, '1593': 41, '692.2': 42, '1407': 43, '724': 44, '575': 45, '318': 46, '1779': 47, '341': 48, '1999': 49, '692.5': 50, '162': 51, '39': 52, '321': 53, '441': 54, '38': 55, '714': 56, '1622': 57, '447': 58, '109': 59, '76.4': 60, '1791': 61, '241': 62, '1611': 63, '698.3': 64, '316.6': 65, '1561': 66, '105': 67, '315': 68, '701': 69, '335': 70, '698.95': 71, '455': 72, '1793': 73, '201.3': 74, '38.5': 75, '724.3': 76, '1434': 77, '233': 78, '774': 79, '720': 80, '717': 81, '458': 82, '531': 83, '81': 84, '31': 85, '1591': 86, '39.9': 87, '1241': 88, '1714': 89, '698.8': 90, '318.5': 91, '692': 92, '503': 93, '18.022': 94, '412': 95, '1031': 96, '1141': 97, '698.4': 98, '1584': 99, '346': 100, '1261.2': 101, '511': 102, '1242': 103, '1729': 104, '1623': 105, '1701': 106, '671': 107, '242': 108, '319.5': 109, '171': 110, '1559': 111, '175.4': 112, '1556': 113, '1472': 114, '1534': 115, '1028.5': 116, '204': 117, '449': 118, '1520': 119, '724.5': 120, '367': 121, '683': 122, '161': 123, '1581': 124, '468': 125, '199': 126, '1567': 127, '1566': 128, '722': 129, '697.5': 130, '1301': 131, '293': 132, '77': 133, '611': 134, '416': 135, '1078': 136, '445': 137, '444': 138, '209': 139, '1042': 140, '125': 141, '1671': 142, '353': 143, '1548': 144, '456': 145, '1275': 146, '108': 147, '1615': 148, '1770': 149, '76': 150, '724.55': 151, '1321': 152, '724.8': 153, '719': 154, '1762': 155, '871': 156, '1681': 157, '713.5': 158, '1571': 159, '469': 160, '905': 161, '57': 162, '481': 163, '724.85': 164, '182': 165, '1546': 166, '1517': 167, '431.5': 168, '423': 169, '1045': 170, '636.6': 171, '636.64': 172, '95': 173, '1571.5': 174, '1768': 175, '818': 176, '51': 177, '1021': 178, '1385': 179, '80.8': 180, '198.7': 181, '1152': 182, '411': 183, '632': 184, '204.5': 185, '376': 186, '21': 187, '789': 188, '1283': 189, '1411': 190, '442': 191, '1558': 192, '204.6': 193, '773': 194, '698.9': 195, '698.7': 196, '1589': 197, '41': 198, '1261': 199, '237': 200, '128': 201, '1311': 202, '353.5': 203, '1676': 204, '1815': 205, '317': 206, '898': 207, '315.5': 208, '1531': 209, '1555': 210, '1773.2': 211, '181': 212, '149': 213, '1051': 214, '638': 215, '88': 216, '1775': 217, '719.5': 218, '76.7': 219, '1613': 220, '1091': 221, '435': 222, '131': 223, '505': 224, '1761': 225, '1389': 226, '1575': 227, '1765': 228, '868': 229, '204.7': 230, '385': 231, '314': 232, '1468': 233, '724.6': 234, '433': 235, '76.6': 236, '327': 237, '183': 238, '365': 239, '123': 240, '2050': 241, '636.7': 242, '1281': 243, '640': 244, '432.5': 245, '39.5': 246, '1724': 247, '809.8': 248, '619.5': 249, '798': 250, '395': 251, '504': 252, '76.8': 253, '1251': 254, '200': 255, '1425': 256, '495': 257, '122': 258, '463': 259, '1098': 260, '1601': 261, '251': 262, '1674': 263, '552': 264, '1025': 265, '1175': 266, '1439': 267, '1522': 268, '1040': 269, '1582': 270, '708': 271, '152': 272, '231': 273, '370': 274, '151': 275, '1503': 276, '1099': 277, '316': 278, '1142': 279, '261': 280, '851': 281, '1777': 282, '357': 283, '576': 284, '1569': 285, '275': 286, '1752': 287, '636.67': 288, '561': 289, '426': 290, '176.5': 291, '110': 292, '698.35': 293, '1868': 294, '1718': 295, '636.3': 296, '1032': 297, '1854': 298, '103': 299, '1563': 300, '64': 301, '1600': 302, '833': 303, '636.68': 304, '1771': 305, '1708.1': 306, '1027': 307, '1573': 308, '201.4': 309, '1565': 310, '635': 311, '245': 312, '133': 313, '1071': 314, '443': 315, '118': 316, '703': 317, '39.2': 318, '1127': 319, '1711': 320, '1795': 321, '591': 322, '80.7': 323, '551': 324, '621': 325, '175.45': 326, '1156': 327, '619': 328, '1462': 329, '75': 330, '449.5': 331, '1728': 332, '11': 333, '1751': 334, '673': 335, '648': 336, '722.3': 337, '928': 338, '482': 339, '1997': 340, '869': 341, '662': 342, '592': 343, '832': 344, '1586': 345, '1879': 346, '175.7': 347, '908': 348, '1612': 349, '39.4': 350, '515': 351, '22': 352, '474': 353, '76.3': 354, '313': 355, '20': 356, '588': 357, '698.2': 358, '471': 359, '203': 360, '778': 361, '1708.2': 362, '727': 363, '724.65': 364, '30': 365, '1683': 366, '1033': 367, '98': 368, '467': 369, '446': 370, '1805': 371, '622': 372, '1404': 373, '712.7': 374, '761': 375, '437': 376, '111': 377, '1429': 378, '891': 379, '731': 380, '56': 381, '40': 382, '1655': 383, '337': 384, '44': 385, '114': 386, '1410': 387, '1171': 388, '755': 389, '630': 390, '645': 391, '734': 392, '1504': 393, '1769': 394, '1679': 395, '145': 396, '141': 397, '176.2': 398, '1713': 399, '172': 400, '475': 401, '1588': 402, '174.4': 403, '1572': 404, '91': 405, '1290': 406, '163': 407, '641': 408, '702': 409, '1413': 410, '1680': 411, '1602': 412, '202': 413, '623': 414, '140': 415, '1412': 416, '1685': 417, '23': 418, '1874': 419, '39.3': 420, '1461': 421, '432.3': 422, '859': 423, '1463': 424, '1331': 425, '565': 426, '32': 427, '1003': 428, '1408.2': 429, '1552': 430, '1702': 431, '1029': 432, '1994': 433, '1038': 434, '1651': 435, '78': 436, '1040.5': 437, '1305': 438, '1408': 439, '1618': 440, '1409': 441, '1861': 442, '1421': 443, '1473': 444, '1445': 445, '1792': 446, '1628': 447, '1878': 448, '1597': 449, '1521': 450, '1533': 451, '1505': 452, '1501': 453, '1691': 454, '1661': 455, '1715': 456, '921': 457, '1291': 458, '1262': 459, '1228': 460, '1272': 461, '1277': 462, '1235': 463, '1378': 464, '1125': 465, '831': 466, '1786': 467, '892': 468, '1343': 469, '893': 470, '852': 471, '187': 472, '573': 473, '1075': 474, '521': 475, '1416': 476, '879': 477, '1721': 478, '1023': 479, '191': 480, '712.5': 481, '1595': 482, '7271': 483, '1442': 484, '853': 485, '885': 486, '1288': 487, '67': 488, '139': 489, '699': 490, '79': 491, '685': 492, '1475': 493, '192': 494, '39.8': 495, '292': 496, '1812': 497, '940': 498, '563': 499, '533': 500, '320': 501, '381': 502, '309': 503, '613': 504, '625': 505, '1211': 506, '825': 507, '1068': 508, '1146': 509, '1143': 510, '1132': 511, '1131': 512, '6985': 513, '188': 514, '418': 515, '733': 516, '732': 517, '751': 518, '676': 519, '672': 520, '1': 521, '634': 522, '1773': 523, '1080': 524, '410': 525, '501': 526, '1028': 527, '1026': 528, '1263': 529, '1471': 530, '1577': 531, '47': 532, '491': 533, '315.2': 534, '595': 535, '592.5': 536, '1052': 537, '582': 538, '1809': 539, '1708.8': 540, '76.72': 541, '605': 542, '693': 543, '1339': 544, '17': 545, '753': 546, '1708.5': 547, '13': 548, '1414': 549, '578': 550, '80.7.A7': 551, '1726': 552, '532': 553, '1596': 554, '1750': 555, '724.2': 556, '1286': 557, '80.7.A78': 558, '663': 559, '501.5': 560, '184': 561, '721.3': 562, '1587': 563, '1679.8': 564, '1621': 565, '150': 566, '1723': 567, '93': 568, '205': 569, '1265': 570, '719.6': 571, '241.7': 572, '375': 573, '112': 574, '858': 575, '294': 576, '93.2': 577, '299': 578, '746': 579, '1758': 580, '698.6': 581, '1550': 582, '107': 583, '132': 584, '1785': 585, '1371': 586, '1553': 587, '636.65': 588, '712': 589, '692.15': 590, '698.55': 591, '1466': 592, '243': 593, '844': 594, '251.6': 595, '1083': 596, '190': 597, '271': 598, '1578': 599, '317.5': 600, '1717.5': 601, '79.5': 602, '1095': 603, '127': 604, '538': 605, '201.5': 606, '811': 607, '1523': 608, '1603': 609, '1381': 610, '901': 611, '724.25': 612, '604': 613, '1444': 614, '692.9': 615, '1801': 616, '505.6': 617, '76.45': 618, '783': 619, '497': 620, '740': 621, '86': 622, '1405': 623, '1030': 624, '1483': 625, '1628.3': 626, '1608': 627, '1716': 628, '1001': 629, '1347': 630, '1352': 631, '1418': 632, '1213': 633, '1092': 634, '821': 635, '431.3': 636, '775': 637, '149.8': 638, '138': 639, '126': 640, '660': 641, '285': 642, '186': 643, '935': 644, '351': 645, '620': 646, '380': 647, '1148': 648, '1121': 649, '1128': 650, '724.55.C63': 651, '728': 652, '115': 653, '4705': 654, '647': 655, '1891': 656, '1524': 657}\n"
     ]
    }
   ],
   "source": [
    "vals = pd.unique(df['Call_Number'])\n",
    "print(len(vals))\n",
    "labels = {value: key for (key, value) in enumerate(vals)}\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset from Training Data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, df):\n",
    "    self.labels = [labels[label] for label in df['Call_Number']]\n",
    "\n",
    "    self.texts = [tokenizer(text,\n",
    "                            padding='max_length',max_length=512, truncation=True,\n",
    "                            return_tensors=\"pt\") for text in df['Title']]\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.labels)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    batch_texts = self.get_batch_texts(idx)\n",
    "    batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "    return batch_texts, batch_y\n",
    "  def classes(self):\n",
    "    return self.labels\n",
    "\n",
    "  def get_batch_labels(self, idx):\n",
    "    return np.array(self.labels[idx])\n",
    "\n",
    "  def get_batch_texts(self, idx):\n",
    "    return self.texts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "15074 1884 1885\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "rand = np.random.randint(1,high=100)\n",
    "print(rand)\n",
    "\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=rand),\n",
    "                                     [int(.8*len(df)), int(.9*len(df))])\n",
    "\n",
    "print(len(df_train), len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "  def __init__(self, dropout=0.5):\n",
    "    super(BertClassifier, self).__init__()\n",
    "\n",
    "    self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.linear = nn.Linear(768, len(vals)) # Change depending on dataset size\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "  def forward(self, input_id, mask):\n",
    "    _, pooled_output = self.bert(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
    "    dropout_output = self.dropout(pooled_output)\n",
    "    linear_output = self.linear(dropout_output)\n",
    "    final_layer = self.relu(linear_output)\n",
    "\n",
    "    return final_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, learning_rate, epoch):\n",
    "  training_dataset, val_dataset = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "\n",
    "  train_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=8, shuffle=True)\n",
    "  val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "  \n",
    "  if torch.has_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "  else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss().to(device)\n",
    "  optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "  \n",
    "\n",
    "  # Results array to store values\n",
    "  results = []\n",
    "  all_train_loss = []\n",
    "  all_val_loss = []\n",
    "  all_train_acc = []\n",
    "  all_val_acc = []\n",
    "\n",
    "  #prev_val_acc = 0\n",
    "\n",
    "  model.to(device)\n",
    "  \n",
    "  for epoch_num in range(epoch):\n",
    "    print(f\"Epoch # {epoch_num+1}/{epoch}\")\n",
    "\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "\n",
    "    for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "      train_label = train_label.to(device)\n",
    "      mask = train_input['attention_mask'].to(device)\n",
    "      input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "\n",
    "      output = model(input_id, mask)\n",
    "\n",
    "      batch_loss = criterion(output, train_label)\n",
    "      total_loss_train += batch_loss.item()\n",
    "\n",
    "      acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "      total_acc_train += acc\n",
    "\n",
    "      model.zero_grad()\n",
    "      batch_loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    \n",
    "    total_acc_val = 0\n",
    "    total_loss_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for val_input, val_label in val_dataloader:\n",
    "        val_label = val_label.to(device)\n",
    "\n",
    "        mask = val_input['attention_mask'].to(device)\n",
    "        input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "        output = model(input_id, mask)\n",
    "\n",
    "        batch_loss = criterion(output, val_label)\n",
    "        total_loss_val += batch_loss.item()\n",
    "\n",
    "        acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "        total_acc_val += acc\n",
    "\n",
    "        # Add values to respective result array\n",
    "        all_train_loss.append(total_loss_train / len(train_data))\n",
    "        all_train_acc.append(total_acc_train / len(train_data))\n",
    "        all_val_loss.append(total_loss_val / len(val_data))\n",
    "        all_val_acc.append(total_acc_val / len(val_data))\n",
    "\n",
    "    print(\n",
    "        f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "        | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "        | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "        | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "    '''\n",
    "    if prev_val_acc > total_acc_val / len(val_data):\n",
    "      print(\"Breaking early because validation accuracy has decreased\")\n",
    "      break\n",
    "    else:\n",
    "      prev_val_acc = total_acc_val / len(val_data)\n",
    "    '''\n",
    "\n",
    "  # Append result arrays to full array\n",
    "  results.append(all_train_loss)\n",
    "  results.append(all_train_acc)\n",
    "  results.append(all_val_loss)\n",
    "  results.append(all_val_acc)\n",
    "\n",
    "  return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1885/1885 [22:55<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.781         | Train Accuracy:  0.037         | Val Loss:  0.727         | Val Accuracy:  0.072\n",
      "Epoch # 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 25/1885 [00:18<23:16,  1.33it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/gregoryknapp/Desktop/BookshelfProject/BookClassification/BERTClassification.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gregoryknapp/Desktop/BookshelfProject/BookClassification/BERTClassification.ipynb#ch0000013?line=4'>5</a>\u001b[0m LR \u001b[39m=\u001b[39m \u001b[39m1e-6\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gregoryknapp/Desktop/BookshelfProject/BookClassification/BERTClassification.ipynb#ch0000013?line=5'>6</a>\u001b[0m EPOCHS \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gregoryknapp/Desktop/BookshelfProject/BookClassification/BERTClassification.ipynb#ch0000013?line=6'>7</a>\u001b[0m model, training_results \u001b[39m=\u001b[39m train(model, df_train, df_val, LR, EPOCHS)\n",
      "\u001b[1;32m/Users/gregoryknapp/Desktop/BookshelfProject/BookClassification/BERTClassification.ipynb Cell 13'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data, val_data, learning_rate, epoch)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gregoryknapp/Desktop/BookshelfProject/BookClassification/BERTClassification.ipynb#ch0000012?line=47'>48</a>\u001b[0m   total_acc_train \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m acc\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gregoryknapp/Desktop/BookshelfProject/BookClassification/BERTClassification.ipynb#ch0000012?line=49'>50</a>\u001b[0m   model\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gregoryknapp/Desktop/BookshelfProject/BookClassification/BERTClassification.ipynb#ch0000012?line=50'>51</a>\u001b[0m   batch_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gregoryknapp/Desktop/BookshelfProject/BookClassification/BERTClassification.ipynb#ch0000012?line=51'>52</a>\u001b[0m   optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gregoryknapp/Desktop/BookshelfProject/BookClassification/BERTClassification.ipynb#ch0000012?line=54'>55</a>\u001b[0m total_acc_val \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/torch/_tensor.py:400\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    392\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    393\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    394\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    398\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    399\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 400\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if 'model' in locals():\n",
    "  del model\n",
    "\n",
    "model = BertClassifier()\n",
    "LR = 1e-6\n",
    "EPOCHS = 5\n",
    "model, training_results = train(model, df_train, df_val, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "  test = dataset(test_data)\n",
    "\n",
    "  test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
    "\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "  if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "  total_acc_test = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for test_input, test_label in test_dataloader:\n",
    "      test_label = test_label.to(device)\n",
    "      mask= test_input['attention_mask'].to(device)\n",
    "      input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "      output = model(input_id, mask)\n",
    "\n",
    "      acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "\n",
    "      total_acc_test += acc\n",
    "  \n",
    "  print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "\n",
    "\n",
    "evaluate(model, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model to Models Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"Models/BERT_Classifier_20_epoch_subjects_only.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model = BertClassifier()\n",
    "load_model.load_state_dict(torch.load(\"Models/BERT_Classifier_20_epoch_subjects_only.pt\", map_location=torch.device('cpu')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
